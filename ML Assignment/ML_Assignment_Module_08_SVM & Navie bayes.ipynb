{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wwMz8M6taBC1"
   },
   "source": [
    "## Question 1: What is Information Gain, and how is it used in Decision Trees?\n",
    "\n",
    "Answer : Information Gain (IG) is a core concept used in training Decision Trees. It is a metric that measures how much \"purity\" or order is gained by splitting a set of data based on a specific feature.\n",
    "1. Definition\n",
    "- Information: In this context, information refers to the reduction in uncertainty or entropy. A dataset with high randomness (e.g., a mix of 50% \"Yes\" and 50% \"No\" labels) has high entropy.\n",
    "- Information Gain: IG calculates the difference between the initial impurity (or entropy) of the data before the split, and the weighted average of the impurity of the two or more resulting subsets after the split.$$\\text{Information Gain} = \\text{Entropy}(\\text{Parent}) - \\sum_{i=1}^{k} \\frac{\\text{Number of Samples in Child } i}{\\text{Total Samples}} \\times \\text{Entropy}(\\text{Child } i)$$\n",
    "2. Usage in Decision TreesInformation Gain is the primary criterion used by algorithms like ID3 and C4.5 to build the tree.\n",
    "- Feature Selection: At every node in the tree, the algorithm tests all available features for the best possible split.\n",
    "- Optimal Split: The feature that results in the highest Information Gain is chosen as the splitting condition for that node. The goal is always to maximize IG, which means finding the split that leads to the purest child nodes (nodes where almost all samples belong to one class).\n",
    "- Tree Growth: This process is repeated recursively on the resulting child nodes until no further gain can be made, or a stopping criterion is met."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "49FYKg_g7dno"
   },
   "source": [
    "## Question 2: What is the difference between Gini Impurity and Entropy?\n",
    "\n",
    "Answer : Gini Impurity vs. Entropy\n",
    "| Feature | Gini Impurity | Entropy |\n",
    "|-----------|-----------|-----------|\n",
    "| Formula Concept | Measures the probability of incorrectly classifying a randomly chosen element in the dataset. | Measures the uncertainty or randomness in a dataset. Based on information theory. |\n",
    "| Range | 0 to 0.5 (0 means pure, 0.5 means maximum impurity for a binary class). | 0 to 1 (0 means pure, 1 means maximum impurity for a binary class). |\n",
    "| Computation | Involves squaring probabilities. Computationally faster as it avoids logarithm calculations. | Involves logarithm calculations. Computationally slower than Gini. |\n",
    "| Goal of Split | Choose the split that results in the lowest Gini Impurity. | Choose the split that results in the highest Information Gain (greatest reduction in Entropy). |\n",
    "| Common Use | Default and preferred metric in algorithms like CART (Classification and Regression Trees), which is used by scikit-learn. | Used historically in algorithms like ID3 and C4.5. |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "afhEZ8uU86ji"
   },
   "source": [
    "## Question 3: What is Pre-Pruning in Decision Trees?\n",
    "\n",
    "Answer : Pre-Pruning (or early stopping) is a technique used to prevent a Decision Tree from growing too large and complex, which helps to avoid the problem of overfitting to the training data.\n",
    "\n",
    "How Pre-Pruning Works\n",
    "\n",
    "Instead of building a full tree and then cutting it back (which is Post-Pruning), Pre-Pruning stops the tree building process early by defining strict rules before or during the splitting process at each node.\n",
    "\n",
    "The algorithm checks these rules at a node before attempting a split:\n",
    "\n",
    "1. Maximum Depth: The tree is not allowed to grow beyond a set number of levels (e.g., maximum depth of 5).\n",
    "\n",
    "2. Minimum Samples Per Split: A node will only be split if it contains a minimum required number of data points (e.g., only split a node if it has at least 20 samples).\n",
    "\n",
    "3. Minimum Impurity Decrease: A split is only performed if it results in a reduction of impurity (like Gini Impurity or Entropy) that is greater than a specified threshold. If the gain from the split is too small, the split is rejected, and the node remains a leaf node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4: Write a Python program to train a Decision Tree Classifier using Gini Impurity as the criterion and print the feature importances (practical)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PqCGfxko9Yfy",
    "outputId": "71e790ca-ccef-486f-c6f0-9294aada459f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Decision Tree Results ---\n",
      "Test Accuracy: 1.0000\n",
      "\n",
      "Feature Importances (Gini):\n",
      "          Feature  Importance\n",
      "petal length (cm)    0.893264\n",
      " petal width (cm)    0.087626\n",
      " sepal width (cm)    0.019110\n",
      "sepal length (cm)    0.000000\n"
     ]
    }
   ],
   "source": [
    "'''Question 4: Write a Python program to train a Decision Tree Classifier using Gini Impurity as\n",
    "the criterion and print the feature importances (practical).'''\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 1. Load Data\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "feature_names = iris.feature_names\n",
    "\n",
    "# 2. Split Data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# 3. Initialize and Train the Model\n",
    "dt_classifier = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
    "dt_classifier.fit(X_train, y_train)\n",
    "\n",
    "# 4. Predict and Evaluate\n",
    "y_pred = dt_classifier.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# 5. Get and Print Feature Importances\n",
    "importances = dt_classifier.feature_importances_\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': importances\n",
    "})\n",
    "importance_df = importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "print(\"--- Decision Tree Results ---\")\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\\n\")\n",
    "print(\"Feature Importances (Gini):\")\n",
    "print(importance_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7iwmFA7b-BZu"
   },
   "source": [
    "## Question 5: What is a Support Vector Machine (SVM)?\n",
    "\n",
    "Answer : A Support Vector Machine (SVM) is a powerful and versatile machine learning algorithm used for both classification and regression tasks. Its strength lies in its ability to handle complex, high-dimensional data.\n",
    "\n",
    "#### Core Concept: The Hyperplane\n",
    "For classification, the primary goal of an SVM is to find the best possible decision boundary, which is called a hyperplane, that separates the data points of different classes.\n",
    "\n",
    "- In 2D data, the hyperplane is simply a line.\n",
    "\n",
    "- In 3D data, the hyperplane is a flat plane.\n",
    "\n",
    "- In higher dimensions, it is a higher-dimensional flat subspace.\n",
    "\n",
    "#### The \"Optimal\" Hyperplane\n",
    "The \"best\" hyperplane is the one that has the largest margin.\n",
    "\n",
    "- Margin: The distance between the hyperplane and the closest data points from either class.\n",
    "\n",
    "- Support Vectors: The data points that lie closest to the hyperplane and determine the position and orientation of the margin are called the Support Vectors.\n",
    "\n",
    "By maximizing this margin, the SVM aims for the best possible separation between the classes, which generally leads to better generalization (less chance of misclassifying unseen data)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pb8ToV0A-gir"
   },
   "source": [
    "## Question 6: What is the Kernel Trick in SVM?\n",
    "\n",
    "Answer : The Kernel Trick is one of the most powerful concepts that makes Support Vector Machines so effective, especially with non-linearly separable data (data that cannot be separated by a single straight line).\n",
    "\n",
    "#### The Problem\n",
    "\n",
    "If the data is non-linear (e.g., data points of one class form a circle around data points of another class), no simple hyperplane (line or plane) in the original low-dimensional space can separate them accurately.\n",
    "\n",
    "#### The Solution: Mapping to Higher Dimensions\n",
    "\n",
    "1. Implicit Transformation: The Kernel Trick uses a kernel function (like the Radial Basis Function, RBF) to mathematically compute the similarity between pairs of data points as if they had been mapped to a much higher-dimensional feature space, without ever explicitly performing the costly calculations of the transformation itself.\n",
    "\n",
    "2. Linear Separation: In this higher-dimensional space, the data points that were tangled up in the low-dimensional space often become linearly separable.\n",
    "\n",
    "3. Efficiency: This \"trick\" allows the SVM to find a linear boundary (a hyperplane) in the high-dimensional space, which corresponds to a complex, non-linear decision boundary when projected back into the original low-dimensional space. This provides a non-linear classifier without the computational complexity of explicitly working with massive, high-dimensional data.\n",
    "\n",
    "The most common kernel functions are Linear, Polynomial, and Radial Basis Function (RBF)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 7: Write a Python program to train two SVM classifiers with Linear and RBF kernels on the Wine dataset, then compare their accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GjfdgwuD-3KK",
    "outputId": "ee044475-7c26-4490-992a-f50e1addf420"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- SVM Kernel Comparison (Wine Dataset) ---\n",
      "Accuracy with Linear Kernel: 0.9815\n",
      "Accuracy with RBF Kernel:    0.9815\n"
     ]
    }
   ],
   "source": [
    "'''Question 7: Write a Python program to train two SVM classifiers with Linear and RBF kernels on\n",
    "the Wine dataset, then compare their accuracies.'''\n",
    "\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# 1. Load and Prepare Data\n",
    "wine = load_wine()\n",
    "X = wine.data\n",
    "y = wine.target\n",
    "\n",
    "# Standardize the data (important for SVM)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# 2. Train SVM with Linear Kernel\n",
    "svm_linear = SVC(kernel='linear', random_state=42)\n",
    "svm_linear.fit(X_train, y_train)\n",
    "y_pred_linear = svm_linear.predict(X_test)\n",
    "acc_linear = accuracy_score(y_test, y_pred_linear)\n",
    "\n",
    "# 3. Train SVM with RBF (Radial Basis Function) Kernel\n",
    "svm_rbf = SVC(kernel='rbf', random_state=42)\n",
    "svm_rbf.fit(X_train, y_train)\n",
    "y_pred_rbf = svm_rbf.predict(X_test)\n",
    "acc_rbf = accuracy_score(y_test, y_pred_rbf)\n",
    "\n",
    "# 4. Compare Accuracies\n",
    "print(\"--- SVM Kernel Comparison (Wine Dataset) ---\")\n",
    "print(f\"Accuracy with Linear Kernel: {acc_linear:.4f}\")\n",
    "print(f\"Accuracy with RBF Kernel:    {acc_rbf:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PlulcyZP_K1z"
   },
   "source": [
    "## Question 8: What is the Naïve Bayes classifier, and why is it called \"Naïve\"?\n",
    "\n",
    "#### Answer : What is the Naïve Bayes Classifier?\n",
    "\n",
    "The Naïve Bayes classifier is a simple yet effective classification algorithm based on Bayes' Theorem from probability theory. It's primarily used for tasks like text classification (e.g., spam filtering, sentiment analysis) and disease prediction.\n",
    "- Bayes' Theorem: This theorem allows the algorithm to calculate the probability of a specific class (e.g., \"Spam\") given a set of features (e.g., the words in the email).\n",
    "$$\\text{P}(\\text{Class}|\\text{Features}) = \\frac{\\text{P}(\\text{Features}|\\text{Class}) \\times \\text{P}(\\text{Class})}{\\text{P}(\\text{Features})}$$\n",
    "The classifier predicts the class that has the highest posterior probability, $\\text{P}(\\text{Class}|\\text{Features})$.\n",
    "#### Why is it called \"Naïve\"?\n",
    "The term \"Naïve\" comes from the major simplifying assumption the model makes, which is often an unrealistically strong assumption in real-world data:\n",
    "\n",
    "- Assumption of Conditional Independence: The model assumes that all the features ($X_1, X_2, X_3, \\dots$) used to predict the class are completely independent of each other, given the class\n",
    "- Example: In spam filtering, the Naïve Bayes model assumes that the probability of the word \"buy\" appearing in a spam email is independent of the probability of the word \"now\" appearing in the same spam email. In reality, these words often appear together, meaning they are dependent.\n",
    "\n",
    "Despite this \"naïve\" and often incorrect assumption, the model performs surprisingly well in many real-world scenarios, particularly because it simplifies the complex probability calculations needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cen6xZjq_ORE"
   },
   "source": [
    "## Question 9: Explain the differences between Gaussian Naïve Bayes, Multinomial Naïve Bayes, and Bernoulli Naïve Bayes.\n",
    "\n",
    "Answer :\n",
    "| Variation | Data Type / Feature Distribution | Common Applications |\n",
    "|-----------|-----------|-----------|\n",
    "| Gaussian Naïve Bayes | Features are continuous numerical values (e.g., height, weight, cholesterol levels). | Assumes the features follow a normal (Gaussian) distribution (bell curve). Used for predicting things like gender, disease presence, or any classification problem with continuous, normally distributed data. |\n",
    "| Multinomial Naïve Bayes | Features represent counts or frequencies (e.g., how many times a word appears in a document). | Best suited for data where the features are discrete counts. Widely used in Text Classification (e.g., spam filtering, document categorization), where the features are word count vectors. |\n",
    "| Bernoulli Naïve Bayes | Features are binary or Boolean (0 or 1, Yes or No, True or False). | Often used in Text Classification where the model only cares if a word is present (1) or absent (0) in a document, regardless of how many times it appears. |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 10: Breast Cancer Dataset Write a Python program to train a Gaussian Naïve Bayes classifier on the Breast Cancer dataset and evaluate accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YAsyFNpICDex",
    "outputId": "0d5fb140-7e54-4411-98ac-6248fafa7763"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Gaussian Naïve Bayes (Breast Cancer Dataset) ---\n",
      "Number of Test Samples: 171\n",
      "Accuracy Score: 0.9357\n"
     ]
    }
   ],
   "source": [
    "'''Question 10: Breast Cancer Dataset Write a Python program to train a Gaussian Naïve Bayes classifier\n",
    "on the Breast Cancer dataset and evaluate accuracy.'''\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# 1. Load Data\n",
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# 2. Split Data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# 3. Standardize Data (Improves performance, especially for GaussianNB)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# 4. Initialize and Train the Model\n",
    "gnb_classifier = GaussianNB()\n",
    "gnb_classifier.fit(X_train_scaled, y_train)\n",
    "\n",
    "# 5. Predict and Evaluate\n",
    "y_pred = gnb_classifier.predict(X_test_scaled)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "# 6. Print Results\n",
    "print(\"--- Gaussian Naïve Bayes (Breast Cancer Dataset) ---\")\n",
    "print(f\"Number of Test Samples: {len(X_test)}\")\n",
    "print(f\"Accuracy Score: {accuracy:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
